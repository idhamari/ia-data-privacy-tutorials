{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOl5Qq2i9KeGDD8jqeEsMbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idhamari/ia-data-privacy-tutorials/blob/main/IA_Ano01_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Guide to Data Privacy\n",
        "\n",
        "**Models, Technologies, Solutions**\n",
        "\n",
        "Vicenç Torra\n",
        "\n",
        "Department of Computing Science, Umeå University\n",
        "Umeå, Sweden\n",
        "\n",
        "**About:**\n",
        "\n",
        "I will try to create a practical tutorial about data privacy using the above book as main reference. I will add python examples so one can understand the concept and I will try to cover background concepts when needed. These notebooks could be used as a course in data privacy or anonymization."
      ],
      "metadata": {
        "id": "AUFKIUB6AV7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction\n",
        "\n",
        "**Shared data** plays a crucial role in driving collaboration and innovation across various fields. For instance, in the healthcare sector, the sharing of patient data among different hospitals and research institutions enables faster and more accurate diagnoses, leading to improved treatment outcomes. By pooling anonymized data from diverse sources, medical professionals can identify patterns and trends, develop new therapies, and make informed decisions for individual patients and public health initiatives.\n",
        "\n",
        "**Public data**, on the other hand, plays a vital role in fostering transparency and empowering communities. For example, government agencies often release public data sets containing information about demographics, education, crime rates, and more. This data enables researchers, policymakers, and citizens to analyze societal challenges, identify areas for improvement, and make data-driven decisions. Public data is instrumental in promoting accountability, supporting evidence-based policies, and encouraging public participation in the democratic process.\n",
        "\n",
        "**Risks:**\n",
        "\n",
        "While sharing data brings numerous benefits, it also entails certain risks that must be carefully managed.\n",
        "\n",
        "One significant concern is the potential compromise of personal privacy and security.\n",
        "\n",
        "For instance, in recent years, there have been several high-profile data breaches where personal information, such as social security numbers and financial details, were exposed to unauthorized individuals.\n",
        "\n",
        " These breaches not only cause harm to individuals but also erode public trust in data sharing practices.\n",
        "\n",
        " Another risk is the misuse of data for unethical purposes. For example, social media platforms collecting user data for targeted advertising have faced criticism for manipulating user behavior and exploiting personal information without explicit consent.\n",
        "\n",
        " It is crucial to establish robust safeguards, such as strong data protection methods, laws and ethical guidelines, to mitigate these risks and protect individuals' rights and interests.\n",
        "\n",
        " **Data anonymization** algorithms are an essential tool in mitigating the risks associated with data sharing.\n",
        "\n",
        " These algorithms transform personal data into a format that cannot be directly linked to an individual, thereby preserving privacy while still allowing for analysis and insights.\n",
        "\n",
        " By implementing effective data anonymization techniques, organizations can minimize the risk of re-identification and unauthorized access to sensitive information.\n",
        "\n",
        "  These algorithms play a crucial role in ensuring that shared data can be used for research, analysis, and innovation while protecting individuals' privacy rights.\n",
        "  \n",
        "  It is vital to strike a balance between data utility and privacy through the use of robust anonymization methods in data sharing practices."
      ],
      "metadata": {
        "id": "OxTH5iVxApuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Privacy enhancing technologies (PET), Privacy-Preserving Data Mining (PPDM), and Statistical Disclosure Control (SDC) are related fields with\n",
        "a similar interest on ensuring data privacy. Their goal is to avoid the disclosure of sensitive or proprietary information to third parties.\n",
        "\n",
        "**Privacy Enhancing Technologies (PET):** refer to a set of tools, methods, and techniques designed to protect individuals' privacy and enhance the security of their personal data. These technologies aim to minimize the risks associated with the collection, storage, processing, and transmission of sensitive information. PETs can be applied to various areas, such as communication networks, data storage, data analysis, and online browsing.\n",
        "\n",
        "**Privacy-Preserving Data Mining (PPDM):** is a specific application of PETs that focuses on protecting privacy during the process of data mining. Data mining involves extracting valuable insights and patterns from large datasets. However, this process often requires access to sensitive or **personally identifiable information (PII)**, which can pose privacy risks.\n",
        "\n",
        "PPDM techniques employ various privacy-preserving algorithms and methods to ensure that data mining can be performed without revealing sensitive information about individuals. These techniques include:\n",
        "\n",
        "1. **Data anonymization:** It involves modifying or removing identifying information from the dataset before analysis. This can be done by replacing identifying attributes with generalizations or using techniques like k-anonymity, l-diversity, or **differential privacy**.\n",
        "\n",
        "2. **Secure multiparty computation:** It enables multiple parties to jointly compute the results of data mining operations without revealing their individual data. Each party encrypts their data, and computations are performed on the encrypted data, ensuring privacy.\n",
        "\n",
        "3. **Homomorphic encryption:** It allows computations to be performed directly on encrypted data without decrypting it, ensuring that sensitive information remains protected during the analysis.\n",
        "\n",
        "4. **Secure data outsourcing:** This approach allows data owners to outsource their datasets to third-party service providers for mining while ensuring the privacy of the data. Encryption and other privacy-preserving techniques are employed to protect the data throughout the outsourcing process.\n",
        "\n",
        "**Statistical Disclosure Control (SDC):** is another set of techniques used to protect privacy by limiting the disclosure of sensitive information when releasing statistical data or research results. Anonymization is considered part of SDC as well. SDC methods aim to find a balance between the utility of the data and the privacy of individuals. These techniques include:\n",
        "\n",
        "1. **Masking:** Sensitive data points are modified by replacing them with similar values to prevent identification while maintaining statistical properties.\n",
        "\n",
        "2. **Aggregation:** Instead of releasing individual-level data, aggregate statistics are provided, such as averages, totals, or proportions. This reduces the risk of re-identifying individuals while still offering useful information.\n",
        "\n",
        "3. **Data swapping and swapping algorithms:** This technique involves swapping data between records or substituting values across different records to break the link between personal identifiers and sensitive information.\n",
        "\n",
        "4. **Sampling and data reduction:** Instead of releasing the entire dataset, a representative sample is used to generate statistical estimates. This reduces the risk of exposing individual-level details.\n",
        "5. **Data Perturbation:** Sensitive data points are modified by adding noise to prevent identification while maintaining statistical properties. Unlike strict anonymization methods that aim to remove or alter identifying information, data perturbation focuses on introducing controlled noise to preserve privacy while maintaining the usefulness of the dataset for statistical analysis. If the noise added to the income values satisfies the requirements of **differential privacy**, then it can be considered a differential privacy mechanism, which is a specific type of anonymization technique. Differential privacy mechanisms ensure that the released data provides privacy guarantees even in the presence of background knowledge and potential attacks.\n",
        "\n",
        "\n",
        "The primary goal of SDC techniques is to ensure that statistical information can be shared for research, policy-making, and decision support without compromising the privacy and confidentiality of individuals involved in the data."
      ],
      "metadata": {
        "id": "fCcjp5N1Au0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Privacy vs Data Security:\n",
        "\n",
        "* **Data Privacy:** methods to prevent when access is authorized e.g. data is public. Attacker has access to the data.\n",
        "\n",
        "* **Data Security** methods to prevent un-authorized access e.g. data is private. Attacker has no access to the data. An example is **cryptography**.\n"
      ],
      "metadata": {
        "id": "c1DFMDjxiXoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data released with privacy issues:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qXMPASMieoSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples:\n"
      ],
      "metadata": {
        "id": "N1YotQJql1Aq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Public data:**\n",
        "\n",
        "**Example1:**\n",
        "\n",
        "A hospital plans to study children's average hospital stay, considering different ages, diagnoses, and changes in length over time. They will analyze data from previous admissions (2010-2019) with limited attributes: year of birth, town, year of admission, illness, and length of stay. However, due to small towns and low population, the table may lead to disclosure, compromising privacy. Simply restricting access or using age at admission instead of birth year and admission year still risks identification.\n",
        "\n",
        "**Example 2:**\n",
        "\n",
        "A university aims to study the impact of studies and commuting distance on student stress. They provide a researcher with records in the format (town, degree, stress leave?). Although seemingly non-personal, records from a town with only one person studying a specific degree can reveal the individual's stress leave status, breaching privacy.\n",
        "\n",
        "**Example 3:**\n",
        "\n",
        "Location data from mobile phones can often identify individuals with just two locations, such as home and workplace. Most people have distinct patterns in their locations, making them easily identifiable.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "These examples illustrate two situations that can lead to disclosure in public data.\n",
        "\n",
        "1. A few unique attributes can easily identify an individual, as seen in the first examples.\n",
        "2. Information richness from online services or other sources can still enable identification despite not having singular attributes."
      ],
      "metadata": {
        "id": "OQdZbwQWl4PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Function of the data is public:**\n",
        "\n",
        "Summary functions of data, such as computing the mean of an attribute, may seem to provide privacy. However, this is not always the case.\n",
        "\n",
        "**Example1**\n",
        "\n",
        "If an attacker is aware of the population mean, such as the average height of people in a certain country being 1.85m, and they discover that the mean height calculated from a dataset is higher, for instance, 1.88m, they can infer the presence of a tall person within the dataset.\n",
        "\n",
        "In this example, we have a population mean height of 1.85m. The dataset contains heights of individuals, and we calculate the mean height from the dataset using np.mean(). If the dataset mean height is higher than the population mean, we conclude that there may be a tall person within the dataset."
      ],
      "metadata": {
        "id": "5Fu8p9VFl58V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Population mean height\n",
        "population_mean = 1.85\n",
        "x = 1.99\n",
        "\n",
        "# Dataset heights\n",
        "# generate random values\n",
        "dataset_heights = [ round(random.randint(184,186)/100,2) for x in range(10)]\n",
        "\n",
        "print(\"dataset_heights: \", dataset_heights)\n",
        "# adding outlier\n",
        "#dataset_heights.append(x)\n",
        "\n",
        "# Calculating the mean height from the dataset\n",
        "dataset_mean = np.mean(dataset_heights)\n",
        "\n",
        "print(\"population_mean : \", population_mean)\n",
        "print(\"dataset_mean    : \", dataset_mean)\n",
        "\n",
        "# Comparing the dataset mean with the population mean\n",
        "if dataset_mean > population_mean:\n",
        "    print(\"There may be a tall person in the dataset.\")\n",
        "else:\n",
        "    print(\"No tall person is inferred from the dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJXrEEduZ5EQ",
        "outputId": "efa14e4f-5e6a-4cbb-a9c9-faaa963f4ebd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_heights:  [1.85, 1.84, 1.86, 1.86, 1.86, 1.84, 1.85, 1.84, 1.84, 1.84]\n",
            "population_mean :  1.85\n",
            "dataset_mean    :  1.848\n",
            "No tall person is inferred from the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example2**\n",
        "\n",
        "Consider computing the mean income of individuals admitted to a psychiatric unit in a city. The incomes are as follows: 1000, 2000, 3000, 2000, 1000, 6000, 2000, 10000, 2000, 4000. The mean income appears to be 3300 Euros, seemingly harmless.\n",
        "\n",
        "However, if we add the income of a rich landlord e.g. 100000, the mean income becomes 12090.90 Euros. This implies that the rich landlord's income is included, allowing inference about this person attendance at the psychiatric unit.\n",
        "\n",
        "Even if the mean were 3300 Euros, inference about the landlord could still be made, suggesting no attendance to the unit.\n",
        "\n",
        "This example highlights that computation, even in the form of a summary, can lead to disclosure. Complex structures built from data, such as machine learning models, can also expose traces of the underlying data, enabling disclosure through attacks like membership inference.\n",
        "\n",
        "Consideration of a linear regression model further exemplifies this issue. Adding the record associated with an outlier value significantly impacts the regression line."
      ],
      "metadata": {
        "id": "jKBHnzPWZ56T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [\"Age\", \"Income\"],\n",
        "     [ 24, 1000],\n",
        "     [ 30, 2000],\n",
        "     [ 40, 3000],\n",
        "     [ 33, 2000],\n",
        "     [ 26, 1000],\n",
        "     [ 40, 6000],\n",
        "     [ 50, 2000],\n",
        "     [ 55, 10000],\n",
        "     [ 37, 2000],\n",
        "     [ 42, 4000],\n",
        "    ]\n",
        "dataX = [\n",
        "    [\"Age\", \"Income\"],\n",
        "     [ 24, 1000],\n",
        "     [ 30, 2000],\n",
        "     [ 40, 3000],\n",
        "     [ 33, 2000],\n",
        "     [ 26, 1000],\n",
        "     [ 40, 6000],\n",
        "     [ 50, 2000],\n",
        "     [ 55, 10000],\n",
        "     [ 37, 2000],\n",
        "     [ 42, 4000],\n",
        "     [ 65, 100000,]\n",
        "    ]\n",
        "\n",
        "y  = [x[1] for x in data[1:]]\n",
        "yX = [x[1] for x in dataX[1:]]\n",
        "\n",
        "n  = len(data[1:])\n",
        "nX = len(dataX[1:])\n",
        "\n",
        "mean = round(sum(y)/n,2)\n",
        "print(\"meanIncome1 : \",mean1)\n",
        "\n",
        "meanX = round(sum(yX)/nX,2)\n",
        "print(\"meanIncome2 : \",mean2)\n",
        "\n",
        "# to check if person in dataset we check the mean with or without him\n",
        "x = 100000\n",
        "\n",
        "meanNoX=round( (nX*meanX-x)/(nX-1) ,2)\n",
        "# we should get the same value i.e. meanIncome1\n",
        "print(\"mean without \",x, \"=\", meanNoX )\n",
        "\n",
        "# This means x found in dataX but not in data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqSX_ykwrlIe",
        "outputId": "d8a1e9a2-4131-43e4-a6e1-ee2d55b64b6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meanIncome1 :  3300.0\n",
            "meanIncome2 :  12090.91\n",
            "mean without  100000 = 3300.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Real-world examples:"
      ],
      "metadata": {
        "id": "nBfg5wk1loco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **AOL case:**\n",
        "\n",
        "In August/September 2006, queries submitted to AOL by 650,000\n",
        "users within a period of three months were released for research purposes. Only\n",
        "queries from AOL registered users were released. Numerical identifiers were published\n",
        "instead of real names and login information. Nevertheless, from the terms in\n",
        "the queries a set of queries was [linked to people](https://www.nytimes.com/2006/08/09/technology/09aol.html)."
      ],
      "metadata": {
        "id": "p51H2vtHffem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Netflix case:**\n",
        "\n",
        "a database with the ratings to films of about 500,000\n",
        "subscribers of Netflix was published in 2006 to foster research in recommender\n",
        "systems. About 100 milion film ratings were included in the database. [Narayanan\n",
        "and Shmatikov](https://ieeexplore.ieee.org/document/4531148) used the Internet Movie Database as background knowledge\n",
        "to link some of the records in the database to known users."
      ],
      "metadata": {
        "id": "nszwnNQafhuy"
      }
    }
  ]
}